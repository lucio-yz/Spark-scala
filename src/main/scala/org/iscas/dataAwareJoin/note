是不是left outer join根本不影响,因为medium表包含了所有key

目前对slice join快的原因不是很清晰

因为虽然划分为了两个bucket,但是默认的hash partitioner,仍然会将某个数据量很大的key划分到固定的partition
这里曾设想使用自定义的partitioner函数,对某个固定的key,返回随机数作为partition id,但是这样显然join的结果不对

似乎是因为划分为两个rdd相当于400个partition
但是尝试将partition数设置为400,运行Normal join,没有加快
这种想法是不对的,实际上划分为两个rdd并没有增加partition数,还是200

实验结果表明,在数据量为900w的情况下,总数为1w个key中有56个key的频率总和能达到0.5

现在将代码调整,以便能看到完整的(不是whole stage codegen)逻辑计划
在打印出了queryExecution之后,看出,性能提升的主要原因是两个rdd的join是broadcast hash join,spark将dfLarge1广播
手动禁止broadcast hash join之后,算法时间就比sort merge join时间长了

发现broadcast是性能提升原因之后,事实上采样这个过程就变的多余了
因为IBJ算法,为数据打上随机的pass,这样缓解了数据倾斜,而我的采样,按照等频划分,实际上没有缓解数据倾斜

因此目前的想法可以总结为数据感知的join算法:

首先采样,统计倾斜的key的数目
如果倾斜的key的数目很多,则使用IBJ
如果倾斜的key的数目较少,则进行随机前缀+扩容的方法,也就是按照倾斜的key,将join的两个rdd分成四个rdd
    对倾斜key的倾斜rdd,加上随机前缀(1~n),对倾斜key的非倾斜rdd,扩容n倍
